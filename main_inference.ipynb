{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('../')\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "from model.DocDiff import DocDiff\n",
    "from schedule.diffusionSample import GaussianDiffusion\n",
    "from schedule.schedule import Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(array):\n",
    "    return (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "def crop_concat(img, size=480):\n",
    "    shape = img.shape\n",
    "    correct_shape = (size*(shape[2]//size+1), size*(shape[3]//size+1))\n",
    "    one = torch.ones((shape[0], shape[1], correct_shape[0], correct_shape[1]))\n",
    "    one[:, :, :shape[2], :shape[3]] = img\n",
    "    # crop\n",
    "    for i in range(shape[2]//size+1):\n",
    "        for j in range(shape[3]//size+1):\n",
    "            if i == 0 and j == 0:\n",
    "                crop = one[:, :, i*size:(i+1)*size, j*size:(j+1)*size]\n",
    "            else:\n",
    "                crop = torch.cat((crop, one[:, :, i*size:(i+1)*size, j*size:(j+1)*size]), dim=0)\n",
    "    return crop\n",
    "\n",
    "def crop_concat_back(img, prediction, size=480):\n",
    "    shape = img.shape\n",
    "    for i in range(shape[2]//size+1):\n",
    "        for j in range(shape[3]//size+1):\n",
    "            if j == 0:\n",
    "                crop = prediction[(i*(shape[3]//size+1)+j)*shape[0]:(i*(shape[3]//size+1)+j+1)*shape[0], :, :, :]\n",
    "            else:\n",
    "                crop = torch.cat((crop, prediction[(i*(shape[3]//size+1)+j)*shape[0]:(i*(shape[3]//size+1)+j+1)*shape[0], :, :, :]), dim=3)\n",
    "        if i == 0:\n",
    "            crop_concat = crop\n",
    "        else:\n",
    "            crop_concat = torch.cat((crop_concat, crop), dim=2)\n",
    "    return crop_concat[:, :, :shape[2], :shape[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = DocDiff(\n",
    "            input_channels=3 + 3,\n",
    "            output_channels=3,\n",
    "            n_channels=32,\n",
    "            ch_mults=[1,2,3,4],\n",
    "            n_blocks=1,\n",
    "        ).to(device) # initialize network\n",
    "\n",
    "\n",
    "schedule = Schedule('linear', 100) # initialize schedule\n",
    "sampler = GaussianDiffusion(network.denoiser, 100, schedule).to(device) # initialize diffusion sampler\n",
    "\n",
    "TEST_INITIAL_PREDICTOR_WEIGHT_PATH = '/home/chirag_tubakad/DocDiff/checksave/model_init_160000.pth'\n",
    "TEST_DENOISER_WEIGHT_PATH = '/home/chirag_tubakad/DocDiff/checksave/model_denoiser_160000.pth'\n",
    "network.init_predictor.load_state_dict(torch.load(TEST_INITIAL_PREDICTOR_WEIGHT_PATH)) # load weights\n",
    "network.denoiser.load_state_dict(torch.load(TEST_DENOISER_WEIGHT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_step:  99\n",
      "time_step:  98\n",
      "time_step:  97\n",
      "time_step:  96\n",
      "time_step:  95\n",
      "time_step:  94\n",
      "time_step:  93\n",
      "time_step:  92\n",
      "time_step:  91\n",
      "time_step:  90\n",
      "time_step:  89\n",
      "time_step:  88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_step:  87\n",
      "time_step:  86\n",
      "time_step:  85\n",
      "time_step:  84\n",
      "time_step:  83\n",
      "time_step:  82\n",
      "time_step:  81\n",
      "time_step:  80\n",
      "time_step:  79\n",
      "time_step:  78\n",
      "time_step:  77\n",
      "time_step:  76\n",
      "time_step:  75\n",
      "time_step:  74\n",
      "time_step:  73\n",
      "time_step:  72\n",
      "time_step:  71\n",
      "time_step:  70\n",
      "time_step:  69\n",
      "time_step:  68\n",
      "time_step:  67\n",
      "time_step:  66\n",
      "time_step:  65\n",
      "time_step:  64\n",
      "time_step:  63\n",
      "time_step:  62\n",
      "time_step:  61\n",
      "time_step:  60\n",
      "time_step:  59\n",
      "time_step:  58\n",
      "time_step:  57\n",
      "time_step:  56\n",
      "time_step:  55\n",
      "time_step:  54\n",
      "time_step:  53\n",
      "time_step:  52\n",
      "time_step:  51\n",
      "time_step:  50\n",
      "time_step:  49\n",
      "time_step:  48\n",
      "time_step:  47\n",
      "time_step:  46\n",
      "time_step:  45\n",
      "time_step:  44\n",
      "time_step:  43\n",
      "time_step:  42\n",
      "time_step:  41\n",
      "time_step:  40\n",
      "time_step:  39\n",
      "time_step:  38\n",
      "time_step:  37\n",
      "time_step:  36\n",
      "time_step:  35\n",
      "time_step:  34\n",
      "time_step:  33\n",
      "time_step:  32\n",
      "time_step:  31\n",
      "time_step:  30\n",
      "time_step:  29\n",
      "time_step:  28\n",
      "time_step:  27\n",
      "time_step:  26\n",
      "time_step:  25\n",
      "time_step:  24\n",
      "time_step:  23\n",
      "time_step:  22\n",
      "time_step:  21\n",
      "time_step:  20\n",
      "time_step:  19\n",
      "time_step:  18\n",
      "time_step:  17\n",
      "time_step:  16\n",
      "time_step:  15\n",
      "time_step:  14\n",
      "time_step:  13\n",
      "time_step:  12\n",
      "time_step:  11\n",
      "time_step:  10\n",
      "time_step:  9\n",
      "time_step:  8\n",
      "time_step:  7\n",
      "time_step:  6\n",
      "time_step:  5\n",
      "time_step:  4\n",
      "time_step:  3\n",
      "time_step:  2\n",
      "time_step:  1\n",
      "time_step:  0\n"
     ]
    }
   ],
   "source": [
    "def perform_inference(img, gt, network, schedule, sampler):\n",
    "    # image pre-processing\n",
    "    seed = torch.random.seed()\n",
    "    torch.random.manual_seed(seed)\n",
    "    # Since the pre-trained weights I provided were padded using the 'reflect' method,\n",
    "    # I need to align the data during testing. Anyway, just remember that aligning the\n",
    "    # training and inference data in the same way is definitely correct.\n",
    "    img = RandomCrop(480, pad_if_needed=True, padding_mode='reflect')(img)\n",
    "    img = ToTensor()(img) # convert to tensor\n",
    "    img = img.unsqueeze(0) # add batch dimension (1, 3, 300, 300)\n",
    "\n",
    "    torch.random.manual_seed(seed)\n",
    "    gt = RandomCrop(480, pad_if_needed=True, padding_mode='reflect')(gt)\n",
    "    gt = ToTensor()(gt) # convert to tensor\n",
    "    gt = gt.unsqueeze(0) # add batch dimension (1, 3, 300, 300)\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "    # temp = img\n",
    "    # img = crop_concat(img) # crop\n",
    "    # with torch.no_grad():\n",
    "    #     noisyImage = torch.randn_like(img).to(device)\n",
    "    #     init_predict = network.init_predictor(img.to(device), 0)\n",
    "    #     sampledImgs = sampler(noisyImage.cuda(), init_predict, 'True') # 128 * 128 sampling\n",
    "    #     finalImgs = (sampledImgs + init_predict)\n",
    "    #     finalImgs = crop_concat_back(temp, finalImgs)\n",
    "    #     init_predict = crop_concat_back(temp, init_predict)\n",
    "    #     sampledImgs = crop_concat_back(temp, sampledImgs)\n",
    "    #     img = temp\n",
    "    #     # img_save = torch.cat((img, gt, init_predict.cpu(), min_max(sampledImgs.cpu()), finalImgs.cpu()), dim=3)\n",
    "    #     save_image(finalImgs.cpu(), '/home/chirag_tubakad/DocDiff/test-images/predicted_images/6938.png')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noisyImage = torch.randn_like(img).to(device)\n",
    "        init_predict = network.init_predictor(img.to(device), 0)\n",
    "        sampledImgs = sampler(noisyImage.cuda(), init_predict, 'True') # full-size sampling\n",
    "        finalImgs = (sampledImgs + init_predict)\n",
    "        # img_save = torch.cat((img, gt, init_predict.cpu(), min_max(sampledImgs.cpu()), finalImgs.cpu()), dim=3)\n",
    "        save_image(finalImgs.cpu(), '/home/chirag_tubakad/DocDiff/test-images-rot/predicted/2008-160K.png') # Due to deterministic sampling, the results should always be exactly the same each time.\n",
    "\n",
    "\n",
    "img = Image.open('/home/chirag_tubakad/DocDiff/test-images-rot/augmented/pdfs2008.png')\n",
    "img = img.convert('RGB')\n",
    "\n",
    "gt = Image.open('/home/chirag_tubakad/DocDiff/test-images-rot/original/pdfs2008.png')\n",
    "gt = gt.convert('RGB')\n",
    "perform_inference(img, gt, network, schedule, sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chartQA",
   "language": "python",
   "name": "chartqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
